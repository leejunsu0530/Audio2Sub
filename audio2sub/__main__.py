import whisper # type: ignore
import srt # type: ignore
import datetime
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn, TaskProgressColumn

# === GPU ì„±ëŠ¥ í™•ì¸ ë° ìµœì  ëª¨ë¸ ì„ íƒ ===
def check_gpu():
    if not torch.cuda.is_available():
        print("âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return "cpu", 0
    mem = torch.cuda.get_device_properties(0).total_memory // (1024 ** 2)  # MB
    print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)} - {mem}MB")
    return "cuda", mem

def load_best_translation_model(device: str, mem_mb: int):
    if device == "cuda" and mem_mb >= 12000:
        model_name = "facebook/nllb-200-3.3B"
        print("ğŸš€ ê³ ì„±ëŠ¥ GPU í™˜ê²½ - nllb-200-3.3B ëª¨ë¸ ì„ íƒ")
    else:
        model_name = "facebook/nllb-200-distilled-600M"
        print("âš™ï¸ CPU ë˜ëŠ” ì¤‘ê°„ê¸‰ GPU í™˜ê²½ - distilled-600M ëª¨ë¸ ì„ íƒ")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
    return tokenizer, model

# === 1. ì˜¤ë””ì˜¤ì—ì„œ ìë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ===
def transcribe_audio(audio_path: str, lang: str = "ja") -> list[dict]:
    model = whisper.load_model("base")
    result = model.transcribe(audio_path, language=lang)
    return result['segments']

# === 2. ì„¸ê·¸ë¨¼íŠ¸ë¥¼ SRT í˜•ì‹ìœ¼ë¡œ ì €ì¥ ===
def segments_to_srt(segments: list[dict], output_path: str):
    subs = []
    for i, seg in enumerate(segments):
        start = datetime.timedelta(seconds=seg["start"])
        end = datetime.timedelta(seconds=seg["end"])
        content = seg["text"].strip()
        subs.append(srt.Subtitle(index=i + 1, start=start, end=end, content=content))
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(srt.compose(subs))

# === 3. SRT ìë§‰ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ë¬¸ë§¥ ê³ ë ¤ + rich í”„ë¡œê·¸ë ˆìŠ¤ í¬í•¨) ===
def translate_srt_to_korean(srt_path: str, output_path: str):
    device, mem = check_gpu()
    tokenizer, model = load_best_translation_model(device, mem)

    with open(srt_path, "r", encoding="utf-8") as f:
        subtitles = list(srt.parse(f.read()))

    texts = [sub.content for sub in subtitles]
    batch_size = 8
    translated_texts = []

    progress = Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeElapsedColumn(),
        TimeRemainingColumn()
    )

    with progress:
        task = progress.add_task("ë²ˆì—­ ì¤‘...", total=len(texts))

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True).to(device)
            ko_id = tokenizer.lang_code_to_id.get("kor") or tokenizer.lang_code_to_id.get("ko")
            outputs = model.generate(**inputs, forced_bos_token_id=ko_id)
            results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            translated_texts.extend(results)
            progress.update(task, advance=len(batch_texts))

    for sub, trans in zip(subtitles, translated_texts):
        sub.content = trans

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(srt.compose(subtitles))
