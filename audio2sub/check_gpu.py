import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from typing import Any, Literal


def check_gpu() -> tuple[Literal['cpu'], Literal[0]] | tuple[Literal['cuda'], Any]:
    if not torch.cuda.is_available():
        print("âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return "cpu", 0

    capability = torch.cuda.get_device_capability(0)
    mem = torch.cuda.get_device_properties(
        0).total_memory // (1024 ** 2)  # MB ë‹¨ìœ„
    print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"    - Compute Capability: {capability}")
    print(f"    - GPU ë©”ëª¨ë¦¬: {mem}MB")
    return "cuda", mem


def load_best_nllb_model(device: str, mem_mb: int):
    # ê³ ì„±ëŠ¥ GPU í™˜ê²½ì´ë©´ í° ëª¨ë¸ ì‚¬ìš©
    if device == "cuda" and mem_mb >= 12000:
        model_name = "facebook/nllb-200-3.3B"
        print("ğŸš€ ê³ ì„±ëŠ¥ GPU í™˜ê²½ - nllb-200-3.3B ëª¨ë¸ ì„ íƒ")
    else:
        model_name = "facebook/nllb-200-distilled-600M"
        print("âš™ï¸ CPU ë˜ëŠ” ì¤‘ê°„ê¸‰ GPU í™˜ê²½ - distilled-600M ëª¨ë¸ ì„ íƒ")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
    return tokenizer, model


def translate_ja_to_ko(text: str, tokenizer, model, device: str) -> str:
    inputs = tokenizer(text, return_tensors="pt", padding=True).to(device)
    ko_id = tokenizer.lang_code_to_id["kor"]
    outputs = model.generate(**inputs, forced_bos_token_id=ko_id)
    translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return translated[0]


# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    device, mem = check_gpu()
    tokenizer, model = load_best_nllb_model(device, mem)

    ja_text = "ä»Šæ—¥ã¯ã¨ã¦ã‚‚æ¥½ã—ã‹ã£ãŸã§ã™ã€‚æ˜æ—¥ã‚‚ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†ã€‚"
    ko_text = translate_ja_to_ko(ja_text, tokenizer, model, device)

    print("\n[ì›ë¬¸]", ja_text)
    print("[ë²ˆì—­]", ko_text)
